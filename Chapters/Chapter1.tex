\chapter{Part One}
\section{Introduction}
This report presents a summary of the doctoral research conducted during my first year
of the PhD study and the plans concerning its progress in the future. The research focuses on studying different methods and challenges in remote teleportation interface design. In remote robotic teleoperation, with the operator manipulating robotic systems from a distance, the quality of perception of the remote environment plays a central role in the accurate estimation of distances, sizes, movements, and spatial orientations in order to execute tasks successfully and efficiently. With the advancements in VR and AR technologies, immersive user interfaces could improve human supervisory control of a teleoperation system by providing visual & sensory feedback and remote environment information in an intuitive and easy-to-understand manner.

Most of the research works focused on providing video feed-backs in monocular and a stereoscopic 360 videos. which only allow operators to look in a fixed direction. Therefore, in this study, I focused my study on Interfaces which can give a freedom of motion in six-degrees-of-freedom (‘6-DoF’), so that operators see the correct views of an environment regardless of where he is (3 DoF) and where he is looking (+3 DoF).

I tried to answer the following research Questions: (i) What are the common difficulties of remote teleoperators when they use monocularvand stereo video interfaces? (ii) Can we improve this difficulties by providing 3D reconstruction of the remote environment? (iii) if they can improve, How can we provide the reconstructed environment in real time with high quality?

\section{Objective}

The main objective of the first part of this Ph.D. research is to investigate how 3D immersive interfaces can be developed to improve visual interactions in remote teleoperation, and to understand the effects of the Visual interfaces operators used ?

The study centers on improving the perception of remote environments in remote robotic teleoperation, using immersive 3DUIs for display and interaction based on the following \textbf{three} main aspects.

\textbf{Risk prevention}

Sensors can detect risky situations for humans such as obstacles, collisions, dangerous areas (fire, gas, etc.), and safe / unsafe terrains. Well-designed interfaces can provide timely sensor feedback and allow humans to perceive the risk before it occurs to be able to avoid it.

%To indicate how the terrain is in front of a navigating robot (safe/ unsafe trains).
%To detect and prevent from a collision.
%To find out dangerous areas (fire, gas, etc.) and alert the operator.\\

\textbf{Task and motion planning}

Task and motion planning requires an operator to perceive the environment, decide how to navigate a region, manipulate objects, etc., before the actual task is performed. The operator requires information on the robot kinematics and capabilities in the remote environment. Interactive interfaces can provide information to the operator in the planning phase, with respect to the possible approaches for executing the tasks.

For instance, the following three tasks can be supported by assistive information for the operator.
\begin{itemize}
    \item To find a trade-off for look-ahead possibilities.
    \item To plan the best navigation/optimal trajectory.
    \item To find the best optimal grasping/manipulation points.
\end{itemize}

\textbf{Human factors}

The learning capabilities and the performance of a human operator is limited by the amount of information they can remember and process \cite{NAP25118}. Limitations such as perception, cognition, and physical ergonomics can be supported by designing immersive interfaces \cite{Bowman:2004:UIT:993837}.\\
To support and guide this study, the following questions will be considered, among others.
\begin{itemize}
    \item How to present information without confusion?
    \item How to find a balance in showing information and for how long to show it?
    \item How to represent the multi-dimensional information (environmental sensors, point-clouds, camera streams, etc.) effectively and efficiently?
    \item How operators perceive different information?
    \item What are the perceptual and cognitive constraints in remote teleoperation?
    \item How to integrate different sensory information in an ergonomic and user-friendly manner, while utilizing human cognitive capabilities?
\end{itemize} 

\section{State of the Art}

%\section {Factors That Affect Remote Teleoperation}
%the General procedures should be what progress in those two months ?
%why this specific literature review needed?
%What I understood ?
%How can we use it ? 
%What limitations are there ?


To reach a comprehensive understanding of the research area, I have been doing a general literature review beginning with the research questions listed below. Figure \ref{Reviewed Papers} shows the total number of papers (155) which are selected from database and publisher websites.

I started my literature review by reviewing papers, which studies about 3D user interfaces and Human factors. These papers cover 16 $\%$ of the total study. The second stage of the literature review was about Augmented reality interfaces for remote teleoperation. it covers  74 $\%$ of the total. The remaining 9.1 $\%$ of the study was about 3D reconstruction for a dynamic environment.

\begin{itemize}
    \item What are the common problems of remote teleoperators when they use monocular and stereo video interfaces?
    \item What is the effect of Augmented Reality Interfaces for remote teleoperation?
    \item How operators interact with field robot with virtually replicated 3d environments?
\end{itemize}

\begin{figure}
     \includegraphics[width=1\textwidth]{images/Reviewed_papers.png}
    \caption{Reviewed Papers}
    \centering
    \label{Reviewed Papers}
\end{figure}

\subsection{Factors that affect remote teleportation}

The first part of the literature review discuss how the human operator process information and what are the different factors that affect remote teleportation: especially related to a field of view (FOV), limited depth perception, network latency and as such.

\underline{Human factors:} Bowman et.al  \cite{Bowman:2004:UIT:993837} refers the term human factors as the capabilities; characteristics; limitations of the human user; and includes considerations related to the body (acting), the sense(perceiving), and the brain(thinking).

A Basic knowledge of how users process information into useful (inter)action is very valuable to understand the human factors and it is called information processing \cite{Bowman:2004:UIT:993837}. Information processing has been studied for many years and different studies have different models. Bowman et al. \cite{Bowman:2004:UIT:993837} adapted a high level staged information processing model from \cite{PMID:11540969} by mapping it into the three main factors: perception, cognition, and physical ergonomics figure \ref{Info Processing}.

According to Bowman et al. \cite{Bowman:2004:UIT:993837} model, a stimuli or events which are perceived will be provided with a meaningful interpretation based on memories of past experiences. In response to what is perceived, actions may get selected, executed or information may get stored in working memory,(short-term memory).

\begin{figure}
     \includegraphics[width=1\textwidth]{images/Info_proc.png}
    \caption{Information processing and related human factors adapted from \cite{PMID:11540969}}
    \centering
    \label{Info Processing}
\end{figure}


\underline { Attention Resources :} Attention can be seen as a selection process and it could be used to draw attention to specific information from the dynamic spatio-temporal environment.

Attention process is prone to errors, which can be raised by certain bottlenecks, which leads to an inability to notice visual (change blindness) and audition changes ( ‘change blindness’ refers to the surprising difficulty observers have in noticing large changes to visual scenes \cite{SIMONS200516}). Similarly, errors occur on a temporal basis, especially when rapid sequences of stimuli occur.

\underline {Short and long term memories:} 
Working memory has a limited capacity and attention resources highly influence it. in contrast, the capacity of our long-term memory is vast; storing information about our world, concepts, and procedures while not being directly affected by attention \cite{Bowman:2004:UIT:993837}.

\underline {Field of View :} 
Cameras with limited angular view create the so-called "Keyhole" effect (a sense of trying to understand the environment with its narrow "Soda Straw" field of view). major consequences of this effect include missing new events, increased difficulty in navigating novel environments, gaps or incoherent models of the explored space \cite{woods2004envisioning}.

\underline {Orientation and Attitude of the robot:} 
When the remote environments become more complex and cues to a robot’s pose become sparser: it becomes easy for a teleoperator using an egocentric (camera) display to lose situational awareness. To successfully navigate locally and Globally to know where the object of interest is, The operator needs to know the robot's attitude (i.e., pitch and roll) to avoid roll-over accidents \cite{wang2004gravity}.

\underline{Orientation in the Remote Environment:} Navigation with a traditional (north-up) map can be challenging at times because of the demand of mental rotation. track-up (ego-referenced; rotating viewpoints) maps consistently show that track-up maps are better for local guidance (i.e., navigation) and north-up maps are better for global awareness \cite{wang2004gravity}.

\underline{Attitude of the robot:} Attitude (i.e., pitch and roll) of a robotic vehicle may be easy to reference when there are other familiar objects (e.g., horizon, buildings, trees, etc.) in the remote environment. However, if those reference points are absent and the onboard cameras are fixed, operators sometimes find it surprisingly hard to accurately assess the attitude of their robotic vehicles \cite{wang2004gravity}.

\underline{Multiple camera and View point:}
The capabilities to see the robot and its local environment gives the operator a better sense of the robot’s location with respect to obstacles, victims, or other potential difficulties \cite{keyes2006camera}. However, the difference in eye point and camera viewpoint could create motion sickness.In addition, when handling multiple robots, it can be challenging for the operator to acquire different contexts rapidly when switching among robots: information in one scene may not be encoded sufficiently to be compared/integrated when accessed subsequently ( change blindness).

\underline{Degraded Depth Perception :}
Degraded depth perception affects the teleoperator’s estimates of distance and size and can have profound effects on mission effectiveness. In the case of monocular cameras, the operators have to rely on other cues: such as shadows, linear perspective and size consistency.

The effect of degraded depth perception has a higher impact when working in unfamiliar and difficult terrain due to lack of apparent size. In addition, remote manipulation that involves ballistic movement, recognition of unfamiliar scenes, analysis of three-dimensionally complex scenes and the accurate placement of manipulators or tools within such scenes \cite{drascic1993investigation}.

\underline{Degraded Video Image and Time delay :}
Degraded video feeds could leave out essential visual cues for building teleoperators mental models of the remote environment. Different factors such as low bandwidth, low frame rate, low resolution and the number of bits per pixel can create degraded video feeds.

\underline{Motion :}
Teleoperation can be difficult and distracting because of the moving platform vibration.which makes viewing the visual displays and manual control/input more challenging \cite{schipani1998quantification}.

\begin{longtable}{p{2.5cm} p{5cm} p{5cm} p{1cm}}
    \hline
      \textbf{Factor} & \textbf{Effects} & \textbf{Suggested solution} & \textbf{Ref.\#}\\
            \hline
            \rowcolor{lightgray} 
           Short and long term memories &
           \begin{enumerate}[noitemsep,nolistsep]
                \item Working memory has a limited capacity.
                \item Long term memory has a slower recall. 
           \end{enumerate} &
           \begin{enumerate}[noitemsep,nolistsep]
           \item Minimize the need for a mighty memory.
           \item Present information in an organized way.
           \item Place all information in close physical proximity.
           \item give the operator control over the information.
           \end{enumerate}
             & 
            \cite{Bowman:2004:UIT:993837}\\
        \hline
          
                Attention resources &
                \begin{enumerate}[noitemsep,nolistsep]
                    \item Working memory is highly affected by attention resources.
                    \item Is prone to errors, which can lead to an inability to notice visual and audition changes.
                \end{enumerate} &
                \begin{enumerate}[noitemsep,nolistsep]
                \item Don't litter the side of the interface with destructible materials.
                \item Focus the operators’ attention on important elements.
                \end{enumerate} &
                \cite{SIMONS200516} \cite{Bowman:2004:UIT:993837} \cite{PMID:11540969}\\
                
                \hline
     \rowcolor{lightgray}
       Decision-Making &
           \begin{enumerate}[noitemsep,nolistsep]
           \item The speed of decision-making processes varies widely, and is affected by conflicts, complexity, or uncertainty of either the information being processed or the detection of the possible outcome.
           
           \end{enumerate} &
            \begin{enumerate}[noitemsep,nolistsep]
            \item Behaviour can be adjusted by emotional state and conditioning.
            \item  Skills allow users to make more accurate and quicker responses.
            \end{enumerate} &  \cite{pavlov1927conditional} \cite{Bowman:2004:UIT:993837} \cite{fitts1967human} \\
            
    \caption{Summery of Human factors}
    \label{table:human factors}
    \end{longtable}

 \begin{longtable}{p{2.5cm} p{5cm} p{5cm} p{1cm}}
    \hline
      \textbf{Factor} & \textbf{Effects} & \textbf{Suggested solution} & \textbf{Ref.\#}\\           
            
        \hline 
        Field of View  &
           \begin{enumerate}[noitemsep,nolistsep]
           \item Restricted FOV affects  target detection and identification.
           \item Distance cues may be lost and depth perception may be degraded.
           \item Degraded remote driving.
           \item Increased difficulty in navigating novel environments.
           \end{enumerate} &
            \begin{enumerate}[noitemsep,nolistsep]
            \item Wider FOV (changeable FOV ) can be used. 
            \item Stereoscopic 3D displays.
            \item Multiple cameras and single cameras with special optics.
            \end{enumerate} &  \cite{Chen2014} \cite{smyth2000indirect} \cite{van2003image} \cite{Chen2007HumanPI} \cite{Suzuki2014} \\
        \hline
          
     \rowcolor{lightgray} Orientation and Attitude of the Robot &
           \begin{enumerate}[noitemsep,nolistsep]
           \item Difficulty knowing the robot orientation in the environment.
           \item North up and tracked-up map.
           \item Mismatch between actual and perceived attitude of robot.
           \item Unawareness of robot's inclination and shape.
           \end{enumerate} &
            \begin{enumerate}[noitemsep,nolistsep]
            \item Track-up map for navigation.
            \item North-up map for tasks involving integration of spatial relations in the environment.
            \item Gravity referenced view.
            \end{enumerate} &  \cite{wang2004gravity} \cite{Chen2007HumanPI} \cite{fitts1967human} \cite{drury2006changing}\\
        \hline
         Multiple camera  and View point &
           \begin{enumerate}[noitemsep,nolistsep]
           \item Attention switching and change blindness.
           \item Motion sickness.
           \item Egocentric: cognitive tunneling.
           \item Exocentric: loss of immediacy and true ground view.
           
           \end{enumerate} &
            \begin{enumerate}[noitemsep,nolistsep]
            \item Auditory alerts.
            \item Multi-modal solutions.
            \item Peripheral cues for egocentric.
            \end{enumerate} &  \cite{keyes2006camera} \cite{Chen2007HumanPI} \cite{olmos2000tactical} \cite{keyes2006camera} \cite{Chen2007HumanPI} \cite{olmos2000tactical}\\
        \hline

        \rowcolor{lightgray} Depth perception &
           \begin{enumerate}[noitemsep,nolistsep]
           \item Underestimation of distance and size.
           \item Degraded navigation, driving, and telemanipulation.
           \end{enumerate} &
            \begin{enumerate}[noitemsep,nolistsep]
            \item Sterioscopic displayes (SDs).
            \item Inter-camera distance should be less than inter-ocular distance.
            \end{enumerate} &  \cite{Bowman:2004:UIT:993837} \cite{fitts1967human} \cite{Chen2007HumanPI}\\
        \hline 
        
        Video frame rate and Time delay.
         &
           \begin{enumerate}[noitemsep,nolistsep]
           \item Degraded motion perception and spatial orientation.
           \item Degraded target identification and latency.
           \item Motion sickness.
           \item Over actuation when delay is variable.
           \end{enumerate} &
            \begin{enumerate}[noitemsep,nolistsep]
           \item Utilize the human cognitive processing speed which is around 170 ms (range: $75-370$ ms).
            \item Augmented reality/ overlaying information.
            \item Predictive Display (simulation in VR/AR).
            \item Robust adaptive algorithm.
            \end{enumerate} &  \cite{madl2011timing} \cite{Chen2007HumanPI} \cite{kebria2019robust} \\
      
        \hline
      \rowcolor{lightgray} Motion &
           \begin{enumerate}[noitemsep,nolistsep]
           \item Degradation on accuracy and latency.
           \item Motion sickness.
           \end{enumerate} &
            \begin{enumerate}[noitemsep,nolistsep]
            \item Multi modal user interface.
            \item Tailor interface to vibratory and motion effects.
            \end{enumerate} &  \cite{kamsickas2003future} \cite{schipani1998quantification}\\
        \hline

\caption{Summery of different factors}
\label{table:tasks}
\end{longtable}

\subsection{Augmented Reality Interface for Teleoperation}
This summery is about my literature review and experiment setups regarding different ways of providing informations using Augmented reality interfaces.

\subsection*{Systematic Literature Review}
My initial objective for this year was to study the state-of-the-art to understand the existing literature in perception interfaces related to teleoperation, 3DUI design, AR etc. This systematic litrature review is part of this objective.

\textbf{Method of research}

To understand in better I have been doing a systematic literature review on Augmented reality interfaces for Teleoperation. To add a depth and guide the study, the following four research questions have been defined.

\begin{itemize}
    \item what are the most suitable immersive interfaces for teleoperation ?
    \item What are the range of applications of AR in Teleoperation ?
    \item How situational awareness is achieved in teleoperation using AR interfaces ?
    \item What are the different challenges of Augmented Reality in Teleoperation ?
\end{itemize}

Papers were identified by means of a structured keyword search on the following major databases and publisher websites.

\begin{itemize}
    \item IEEE Xplore (http://ieeexplore.ieee.org/)
    \item Scopus (www.scopus.com).
    \item ScienceDirect (https://www.sciencedirect.com/)
    \item ACM (https://dl.acm.org)
\end{itemize}\\

Keywords such as \textbf{Teleoperation; telepresence; Augmented Reality;Mixed Reality; teleoperator ; telerobotics} were combined as such.
\begin{itemize}
    \item "Teleoperation " AND ("Augmented Reality" OR "Mixed Reality").
    \item ("Teleoperation" OR "Teleoperator") AND ("Augmented Reality" OR "Mixed Reality").
    \item ("Teleoperation" OR "Telerobotics") AND ("Augmented Reality" OR "Mixed Reality").
\end{itemize}

However, In order to align the research papers with the research aims inclusion and exclusion criteria  defined by language(only articles in English) and restriction by primary articles (excluding secondary studies such as other systematic reviews, area mapping and surveys).

The over all sample papers considered in this study are 508 (IEEE=96, ACM=101, Scopus=211, Sciencedirect=100):before applying the exclusion criteria) and 139 research papers after applying the exclusion criteria.

The time distribution of the papers published shown in figure \ref{fig:Reviewed Papers}. A small growth can be seen between 1993 and 2009 in the number of papers, however, slight growth appeared on 2013 with gradual changes to the current years, rationalizes the rise of the importance of the topic of Augmented reality for Teleoperation in recent years.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Selected_papers.png}
    \caption{Number of Selected papers}
    \label{fig:PapersNumber}
\end{figure}

To characterise the papers, a bibliometric analysis of these initial 508 research papers and the final set 139 research papers was performed by using VOSviewer software and it reveals the most frequent keywords. To obtain more information about these keywords, a colour-coded graph of their density was produced \ref{fig:final Density}. The colour distribution is between blue and red. The greater the number of items near a point and the greater the weight of neighbouring items, the closer the keyword is to red. On the contrary, the smaller the number of items in the neighbourhood of a point and the smaller the weights of neighbouring items, the closer the keyword is to blue.keywords such as 'Augmented Reality','Teleoperation','Telerobotics','Navigation','Mixed Reality' and 'Telepresence' appeared with the greatest density.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/Density_3.png}
    \caption{Mapping of the most frequent keywords of the final set of research papers}
    \label{fig:final Density}
\end{figure}

To investigate the range of applications of AR in Teleoperation the final set of the papers were verified. Figure \ref{fig:range of applications} shows the 4 broad categories that appear most.
 
To define the  major categories, I adapted the procedure used in \cite{7912316} and I categorized them into navigation, manipulation, simulation, and virtualization categories.
 
The first categorie is Navigation, which includes papers used for Localization, geonavigation, geovisualizations of interactive maps, landscape visualizations, to indicate safe navigation and for free view points indication.this are around 43 papers, about 37.4 $\%$ of the papers in the set.
 
The second categorie is manipulation, application such as overlaying information, information about the target and instructions for task execution are categorized together. 48 of the papers, about 41.7 $\%$ of the papers in the set.
 
Papers which use AR for Flight and drive simulation using surrogates, for collision detection before deployment and task planning are categorised in the group Simulation.15 paper , about 13 $\%$ of the papers in the set.
 
The remaining 9 papers, about 7.8$\%$. used for conference and training application are classified as a Virtualization group.
 
 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/chart.png}
    \caption{Range of applications where AR is used in teleoperation}
    \label{fig:range of applications}
\end{figure}

\subsection{Real-time 3D Reconstruction for Teleoperation}

In this section I was exploring the representation of remote environment (In 3D point clouds and Mesh) and convincingly reconstruct the visual dynamics of the real world in real-time and provide it in intuitive way. For e.g., showing the 3D reconstruction of the environment to the operator in real-time as the remote robot moves through the field.

\textbf{Static 3D Reconstruction}

3D reconstruction of static environments has its roots in several areas within computer vision and graphics.Although there are many different algorithms for RGB-D reconstruction of static scenes, most of these approaches have a very similar processing pipeline \cite{Zollhofer2018}, which I describe here for reference \ref{fig:staticrec}.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{images/StaticRecon.PNG}
    \caption{3D reconstruction for static environment pipeline}
    \label{fig:staticrec}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{images/static_failure.png}
    \caption{Static environment reconstruction failure. \cite{Keller:2013:RRD:2544744.2544784}}
    \label{fig:staticfailure}
\end{figure}

In the first stage, the Depth Map Preprocessing, noise reduction and outlier removal is applied to the incoming RGB-D data. Depending on the following stages, additional information is derived from the input range map and stored in additional input maps. In the subsequent stage, the Camera Pose Estimation, the best aligning transformation for the current frame is computed. This can be achieved in a frame-to- frame, frame-to-model, or global fashion. Finally, all points from the current input frame are transformed with the estimated transformation and are merged into the common model in the Depth Map Fusion stage \cite{Zollhofer2018}.

3D Reconstruction for static environments \textbf{assumes the environment is static and moving objects detected as an outlier and ignored}. in addition this it can create a failure of camera tracking see figure \ref{fig:staticfailure}. This could create a Miss- perception of the scene and can create a confusion to the operator.

\textbf{Dynamic 3D Reconstruction}

In addition to static components, many natural environments contain dynamic objects. for such reasons a real-time-dynamic environment reconstruction system is needed. More recently, many people have begun to work on this area. The approach works by classifying the dynamic scene into static and dynamic parts.then, the camera motion and the dynamic parts of the scene are segmented and reconstructed separately figure\ref{fig:dynamic}.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.4]{images/DynamicRecon.png}
    \caption{3D reconstruction for dynamic environment pipeline.}
    \label{fig:dynamic}
\end{figure}

Figure \ref{fig:dynamicscene} show a reconstruction of a moving person. The motion of the person leads to a failure of camera tracking figure \ref{fig:staticfailure}. The approach \cite{Keller:2013:RRD:2544744.2544784} computes a foreground segmentation based on scene dynamics (A) and excludes them from camera pose estimation (B). This enables robust camera tracking even if large parts of the scene is moving (bottom). 

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{images/Dynamic_rec.PNG}
    \caption{Dynamic 3D reconstruction of a dynamic scene.Image taken from \cite{Keller:2013:RRD:2544744.2544784} }
    \label{fig:dynamicscene}
\end{figure}\\

More recently, many people have begun to leverage Deep Neural Networks and their ability to learn from large amounts of training data to improve Dynamic 3D reconstruction.They categorised this approaches into three main directions.One deforms the whole world in a non-rigid manner in order to include a moving object \cite{newcombe2015dynamicfusion}.The second specifically aims at building a single static background model while ignoring all possibly moving objects and thus improving the accuracy of camera tracking \cite{jaimez2017fast} \cite{scona2018staticfusion} \cite{barnes2018driven}\cite{bescos2018dynaslam}.The third models dynamic components by creating sub-maps for every possibly rigidly moving object in the scene while fusing corresponding information into these sub-maps \cite{runz2017co} \cite{barsan2018robust}\cite{runz2018maskfusion} \cite{xu2019mid} \cite{narita2019panopticfusion} \cite{ miksik2019live} \cite{strecke2019fusion} \cite{hachiuma2019detectfusion}.

The reconstruction of dynamic scenes is \textbf{computationally and algorithmically more challenging } than its static reconstruction counterpart. Modeling the non-rigid motion of general deforming scenes requires orders of magnitude more parameters than the static reconstruction problem. In general, finding the optimal deformation is a high-dimensional and highly non-convex optimization problem that is challenging to solve, especially if \textbf{real-time performance} is the target (state of the art dynamic 3D reconstruction works at a maximum of 5Hz \cite{inproceedings}).

\section{Report on progress}

Remote  teleoperation  relies  on  communication  networks,  with  limited  bandwidth,  low  frame  rates,  low  resolution,  etc.  These  aspects  result  in  degraded  video  feeds  and  point-clouds,  which  can  leave  out essential visual cues for the operator’s mental models of the remote environment. As noted above, in VR-based teleoperation, the operators rely heavily on the visual cues to understand their location, and their relative range with respect to the task space. 
As part of my initial study , I have been designing and experimenting different methods of low latency video streaming , marker based Augmented Reality interface design , implementing 3d Reconstruction for static environment and Point Cloud Streaming from multiple depth sensors.

\textbf{Augmented Reality Interface}

One way to restore the visual cues is to enhance the VR scene itself by overlaying the objects from the real-world environment with the corresponding virtual 3D meshes in the VR scene. The AR interface, as  described  in  [1],  can  help  the  operator  overcome  the  difficulties  of  perception  by  guiding  the  operator using virtual overlays in their view. Marker-based AR algorithms can be used, based on pre-defined “marker” patterns, to register the overlays in the operator view. In the context of VR-based teleoperation, the marker-based AR interface can enhance the VR scene by registering the 3D meshes of the identified objects in the scene. Figure 19 shows the concept with the real-world scene, the virtual scene,  and  the  3D  meshes  overlaid.  The  example  task  shown  here  is  the  peg  attached  as  the  end-effector of the UR5, to be inserted in the hole located on the wall. The 3D meshes of the peg and the hole are seen in the 


\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{images/Aruco_AR.PNG}
    \caption{Augmented Reality  interface  for  scene enhancement of the remote environment and virtual fixture based guided teleoperation.}
    \label{fig:Aruco_AR}
\end{figure}\\


Associating the real-world objects with the 3D meshes is possible by attaching markers to the objects of  interest  in  the  physical  world  and  by  extracting  their  identifier  in  the virtual  world.  A  popular  approach  for  this  is  to  use  square  fiducial  markers  (ArUco  markers),  shown  in  Fig. \ref{fig:Aruco_AR} top right  image.  This  marker is a synthetic square marker composed of a wide black border and an inner binary matrix that forms its identifier. Custom code has been developed that provides acquisition of videos from the cameras as well as camera pose tracking using the ArUco markers. The following stages are involved in the process:

\begin{itemize}
    \item Based on known objects in the real-world, 3D meshes of the objects are generated, which can be used as overlays within the VR scene. 
    \item Printed AruCo markers are placed next to the objects in the real-world, in such a way as to be visible in the camera view. Each object gets a unique identifier for the marker.
    \item The  relative  position,  orientation,  and  the  identifier  of  the  marker  are  calculated  using  the  OpenCV libraries. Based on the extracted identifier, the corresponding 3D mesh is introduced in the virtual scene. 
\end{itemize}


Two different methods have been adopted to assist the operator using the 3D mesh in the VR scene.

\begin{itemize}
    \item \textbf{Point-cloud  enhancement:}  As  seen  in \ref{fig:Aruco_AR},  the  point-cloud  is  visible  to  the  operator  within the VR scene, allowing depth information when interacting with objects. With this AR interface,  the  3D  meshes  can  be  overlaid  directly  over  the  point-cloud at  the  position  and  orientation  of  the  real-world  object.  This  is  seen  in  Fig. \ref{fig:Aruco_AR} bottom right,  where  the  3D  mesh  is  visible  clearly, in addition to point-cloud. This method allows the operator to see the complete object in  VR,  instead  of  just  the  parts  of  it  coming  from  the  point-cloud.  Additionally,  the  mesh  remains in place even if the point-cloud is turned-off.
    
    \item \textbf{Virtual fixture tool:} This is an independent way of visualizing the meshes in the VR scene. It provides a close-up view of the 3D objects of interest, in   this case, the peg and the hole, in a separate space in the VR scene (Fig. \ref{fig:Aruco_AR} bottom left). As a virtual fixture, the tool can be zoomed into or out,  rotated,  and/or  scaled  to  allow  better  understanding  of  the  target  task  and  closer  visualization of the interacting objects. With fluid mapping between the teleoperator and the VR scene, as described above, this eliminates the need of “teleporting”, i.e., moving around in the virtual space.
    
\end{itemize}

In either case, the position and orientation of the 3D meshes in the VR scene need to be updated in real-time based on the calculated translation and rotation of the markers in the camera view.

\textit{Analysis: What are the challenges?}
\begin{itemize}
    \item Markers doesn't exist in real-World.
    \item Pri designed 3D models doesn't represent the actual state of the world.
    \item limited to a certain number of objects.
\end{itemize}


\textbf{Multiple camera and Point-cloud Streaming}

One of the important requirement of remote teleoperation is to have low latency, high-quality video and point cloud streaming between the MASTER and FIELD robot with low bandwidth consumption. To meet this requirement I have designed multiple cameras and point-cloud streaming algorithms on Jetson Xavier. The Xavier incorporates hardware acceleration (GPU) for encoding various coding standards using its NVIDIA Multi-Standard Video Encoder.

Figure \ref{fig:StreamingPipe} shows A Video and point cloud streaming architecture with detail components pipeline. Point cloud and RGB data, which are acquired from the two Real sense (RS 1 and RS 2) and ZED (ZED 1 and ZED 2) cameras are processed before given to streaming pipeline. Subsequently, a multithreaded algorithm splits it into video and point cloud data and supplies it into the two streaming pipelines simultaneously. 

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{images/Video_Point_Streaming.PNG}
    \caption{A Video and point cloud streaming architecture with detail pipeline.}
    \label{fig:StreamingPipe}
\end{figure}\\


\begin{figure}[h]
    \centering
    \includegraphics[scale=0.3]{images/Point_cloud_multipl_1.PNG}
    \caption{A Video and point cloud display in virtual reality.}
    \label{fig:StreamingPipe}
\end{figure}\\


The video streaming pipeline uses video transcoding components that involve a three-step process. First, the video file is converted to an uncompressed format. Second, this uncompressed format is then encoded into the target format and finally streamed to the MASTER station. The detail description of each component is listed below:
\begin{itemize}
    \item \textbf{Converter:} This component converts from one color space (e.g. RGB) to another one (e.g. YUV). It can also convert between different YUV formats (e.g. I420, NV12, YUY2 …) or RGB format arrangements (e.g. RGBA, ARGB, BGRA…).
    \item \textbf{Encoder:} This component encodes the raw video coming from the convert element into H264 compressed data, also otherwise known as MPEG-4 AVC (Advanced Video Codec). The element uses a hardware-based encoder (referred to as NVENC) [1] , which provides fully accelerated hardware-based video encoding.
    \item \textbf{Streamer:} We have implemented a streaming protocol, which uses Real-time Transport Protocol (RTP) to deliver the video over IP networks to the master station. The Stream element uses a gstreamer udpsink that sends UDP packets to the network.  

\end{itemize}


Point-cloud streaming pipeline acquires 3D data from ZED and real-sense camera. Which consist of huge point sets describing three-dimensional points associated with additional information such as distance, color, normals, etc. with high rate and occupy a significant amount of memory resources and bandwidth. To transmit this data over rate-limited communication channels, methods for filtering noise and compressing are designed.The detail description of each component is listed below: 

\begin{itemize}
    \item \textbf{Generate 3D Point clouds:} This component generates takes 3D depth data from the Realsense and ZED camera and generates 3D point-clouds that have similar data type format with Point Cloud Library standard format.
    \item \textbf{Filter Noise:}This component filters some noise and outliers, which are coming from measurement errors, some frames, present a large number of shadow points. This complicates the estimation of local point cloud 3D features. Some of these outliers can be filtered by performing a statistical analysis on each point's neighborhood, and trimming those, which do not meet a certain criteria.
    \item \textbf{Compression:} This component exploits the spatial temporal redundancies in point clouds to compress the 3D point cloud. This component uses the PCL Octree based compression codec [2], which is based on the octree data structure and uses an inter-frame XOR-based codec for temporal compression.
    \item \textbf{Stream:} This component uses the Boost Asio C++ network library and the library is portable and works across most operating systems, and scales well over multiple concurrent connections.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.3]{images/wireshark_general.jpg}
    \caption{packets measured from stream}
    \label{fig:StreamingPipe}
\end{figure}\\

\textit{Analysis: What are the challenges?}
\begin{itemize}
    \item Point clouds are very bulky! (573001 points).
    \item Highly resource intensive.
    \item Low  data transmission rate.
    \item Only compresses geometry.
\end{itemize}


\textbf{Real-time 3D reconstruction with Point-cloud Fusion}

The implemented video feedback from monocular and stereoscopic videos in VR headsets allow operators to look in a fixed direction, but these video interfaces do not respond to any head motion such as moving left/right, forward/backward or up/down. Truly immersive Interface, on the other hand, requires ‘freedom of motion’ in six-degrees-of-freedom (‘6-DoF’), so that operators see the correct views of an environment regardless of where he is (3 DoF) and where he is looking (+3 DoF).

I have integrated a 3D reconstruct algorithm, which reconstructs the FIELD environment in real-time and Showing the 3D reconstruction of the environment to the operator in real-time as the remote robot moves through the field. As a result, the operator could navigate the FIELD environment with much better freedom of motion.


The algorithm is based on the state of the artwork real-time dense visual SLAM algorithm Elastic Fusion [3]. The system is capable of capturing comprehensive dense globally consistent surfel-based maps of an environment and beyond explored using an RGB-D camera in an incremental online fashion, without pose graph optimization or any post-processing steps. This is accomplished by using dense frame-to model camera tracking and windowed surfel-based fusion coupled with frequent model refinement through non-rigid surface deformations.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{images/Elastic_Fusion.PNG}
    \caption{Real-time 3D reconstruction of static environment for teleoperation.}
    \label{fig:Elastic}
\end{figure}\\


The algorithm applies local model-to-model surface loop closure optimizations as often as possible to stay close to the mode of the map distribution while utilizing global loop closure to recover from arbitrary drift and maintain global consistency. In the spirit of improving map quality as well as tracking accuracy and robustness, the algorithm furthermore, explore a novel approach to real-time discrete light source detection.

Figure \ref{fig:Elastic} shows 3D reconstructed environment with the corresponding camera and robot position, the stereo camera mounted on the robot end-effector provides the real-time point cloud of the scene and   the algorithm [3] fuses them to generate the full 3D reconstruction of the environment.

\textit{Analysis: What are the challenges}
\begin{itemize}
    \item Only works for static environment.
    \item Highly resource intensive..
    \item Low Quality reconstruction.
\end{itemize}

\section{Research Plan}

During the first PhD year I have focused on researching the state-of-the-art to understand the limitation of 3D immersive interface design for remote teleoperation. In particular I focused on Augmented reality interfaces and real-time 3D reconstruction. In addition for a deeper understanding of the problem I planned to implement the some applications.

The work plans are presented in the Table \ref{table:plan} shown below and it has been distributed to make it consistent with literature review , where the end of literature review and the end of each project considered as a milestone.

\begin{table}[ht] 
\begin{tabular}{ p{4.5cm} p{10cm}}
 \hline
% \multicolumn{4}{|c|}{Country List} \\
% \hline
 \rowcolor{lightgray}  Duration & Research and project \\
\hline

 1 October - 13 December &  Literature review on factors which affect remote tele-operation. \\

\rowcolor{lightgray} 1 October - 31 December &  Implement low latency video streaming. \\

 28 December - 23 March &  Literature review on Augmented Reality interface. \\

\rowcolor{lightgray} 5 February - 23 April: & Implement Augmented Reality Interface. \\

 24 April - 19 July & Literature review on Real-time 3D reconstruction. \\

\rowcolor{lightgray} June 17 &  PhD day poster presentation.\\

 1 June - 31 July & Implement Point-cloud streaming framework.\\
 
 \rowcolor{lightgray} 1 August - 20 September &  Integrate the applications.\\

\end{tabular}
\caption{A list of research and projects for first year}
\label{table:plan}
\end{table}

\subsection*{Future Research Plan}

Following the literature review and the projects have implemented so far, the research will continue to study  how users engage in virtually replicated 3D models of environments, to understand the effect of different artifacts, an experiment will be designed to include different levels of 3D model representations by changing the fidelity (resolution/density) of the models being displayed and the number of components. alongside a real-time dynamic environment 3D reconstruction algorithm will be implemented. The results from this experiment will also help me to understand more on how to design efficient algorithms that increase the level of engagement to the user.

Following this study, I will design a systematic region selection mechanisms to select relevant and irrelevant information to prioritize computational demand. one of the methods to select region is to use A “Visual attention” mechanisms which is inspired by human visual system will be used to select important areas of the visual field \textbf{(for alerting)} the operator and to search a target in cluttered scenes \textbf{(searching)}. 

This Visual attention model will be designed by taking inputs from the operator, task, sensors and the mission(goal). 

\begin{itemize}
    \item \textbf{Operator}: Operators navigate and perform a remote task by carefully monitoring the environment using Interfaces such as head-mounted displays.Understanding where the operator's processing priority (Attention) is allocated in the scene can be used to improve the performance. one of the methods to understand the operator's region of interest is by using eye trackers. which offers an important opportunity to examine more directly where attention is deployed, in both navigation and executing the task.
    \item \textbf{Task:} A task is an activity or part of work which has to be done, knowledge about the task will help us to select a subset of the available information for further processing (improvement).
    \item \textbf{Sensors:} Sensors are in charge of sensing the different variables or states of the remote system that include both the devices and the environment. when some important information appears sensors can be used to provide/ redirect the attention to the system.
    \item \textbf{Goal:} navigation to a goal, including from starting locations to the destination can be helpfull to select and improve the enviroment. 
\end{itemize}

Figure \ref{fig:Attention} shows a systematic diagram of the system model,the attention model will decide the priority of instances based on the knowledge mentioned above and a higher computational power will be given to the instances based on there associated weight. doing this the system will have enough computational power to process each important instances with better quality. moreover the attention model will help the communiation delay by only streaming the important and neccesory instances to the master station with out bottlneks. 

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.55]{images/Dyn_Reco_Attention.PNG}
    \caption{Real-time 3D reconstruction of dynamic environment using attention models}
    \label{fig:Attention}
\end{figure}\\

%In the near future, I will continue to develop instance level 3D reconstruction algorithm for dynamic environment. Once these algorithm established,  I will carry on studying How operators navigate and interact with virtually replicated 3D environment to understand and model an attention model shown in Figure \ref{fig:Attention}.

%In the long-term,  I will complete the development of Attention aware dynamic environment 3D reconstruction algorithm. 

Finally, I will combine the attention model with the Dynamic 3D reconstruction model, generalise the proposed model and strategies and conduct an empirical evaluation and theoretical analysis on the final version.

\textbf{\textit{Year Two: October 2019 - September 2020}}
\begin{itemize}
    \item Conduct user-study on how users engage in virtually replicated 3D models of environments.
    \item Conduct experiments and collect data.
    \item Write a research paper about the experimented results.
    \item Implement real-time dynamic environment 3D reconstruction algorithm.
    \item Design Attention model based on the results of the study.
    \item Combine the attention model with the Dynamic 3D reconstruction algorithm.
    \item Evaluate the Implemented algorithms for computational efficiency and Quality. 
    \item Refine the total system according to the evaluation results.
    \item Conduct experiments and compare with other approaches.
    \item Write a research paper about the experimented results.
\end{itemize}


\textbf{\textit{Year Three: October 2020 - September 2021}}
\begin{itemize}
    \item Explore if the designed algorithm is suitable for point-cloud compression.
    \item Conduct experiments and compare with other approaches.
    \item Write a research paper about the experimented results
    \item Write thesis & Defend.
\end{itemize}